{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "# import sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"DATABASE_TYPE = 'postgresql'\\nDBAPI = 'psycopg2'\\nENDPOINT = 'justgiving-scraper.czdhldofvzrb.eu-west-2.rds.amazonaws.com' # Change it for your AWS endpoint\\nUSER = 'Raghul_Mac'\\nPASSWORD = 'Raghul123'\\nPORT = 5432\\nDATABASE = 'database-1'\\n\\nBUCKET_NAME = 'justgiving-scraper'\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''DATABASE_TYPE = 'postgresql'\n",
    "DBAPI = 'psycopg2'\n",
    "ENDPOINT = 'justgiving-scraper.czdhldofvzrb.eu-west-2.rds.amazonaws.com' # Change it for your AWS endpoint\n",
    "USER = 'Raghul_Mac'\n",
    "PASSWORD = 'Raghul123'\n",
    "PORT = 5432\n",
    "DATABASE = 'database-1'\n",
    "\n",
    "BUCKET_NAME = 'justgiving-scraper'\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    def write_to_rds(self, data) -> None:\\n        \\n        rds_client = sqlalchemy.create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}\")\\n        rds_client.connect()\\n\\n\\n        sql_query = f\"SELECT * FROM fundraisers WHERE Recipe_Name = \\'{data[\\'Recipe_Name\\']}\\'\"\\n        duplicates = rds_client.execute(sql_query).fetchall()\\n        \\n        if len(duplicates) == 0:\\n            data_df = pd.DataFrame(data, index=[str(uuid4())])\\n            data_df.to_sql(\\'fundraisers\\', rds_client, if_exists=\\'append\\')\\n\\n\\n        return None'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AWS:\n",
    "\n",
    "    def upload_file_method(self, file_name, object_name) -> str:\n",
    "       \n",
    "        s3_client = boto3.client('s3')\n",
    "        BUCKET_NAME = 'dcpproject'\n",
    "        \n",
    "        try:\n",
    "            s3_client.get_object(Bucket=BUCKET_NAME, Key=object_name)\n",
    "            \n",
    "        except:\n",
    "            s3_client.upload_file(file_name, BUCKET_NAME, object_name)\n",
    "\n",
    "        file_url = f's3://{BUCKET_NAME}/{object_name}'\n",
    "        \n",
    "        return file_url\n",
    "'''\n",
    "    def write_to_rds(self, data) -> None:\n",
    "        \n",
    "        rds_client = sqlalchemy.create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}\")\n",
    "        rds_client.connect()\n",
    "\n",
    "\n",
    "        sql_query = f\"SELECT * FROM fundraisers WHERE Recipe_Name = '{data['Recipe_Name']}'\"\n",
    "        duplicates = rds_client.execute(sql_query).fetchall()\n",
    "        \n",
    "        if len(duplicates) == 0:\n",
    "            data_df = pd.DataFrame(data, index=[str(uuid4())])\n",
    "            data_df.to_sql('fundraisers', rds_client, if_exists='append')\n",
    "\n",
    "\n",
    "        return None'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.aws = AWS()\n",
    "\n",
    "    def load_page(self, url) -> None:\n",
    "        \n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "        options = Options()\n",
    "        options.add_argument('--headless')\n",
    "\n",
    "        self.driver = webdriver.Chrome(options=options) \n",
    "        self.driver.maximize_window()\n",
    "        self.driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        self.accept_cookies()\n",
    "\n",
    "        return None\n",
    "\n",
    "    def accept_cookies(self) -> None:\n",
    "        try:\n",
    "             # This is the id of the frame\n",
    "            accept_cookies_button = self.driver.find_element(by=By.XPATH, value='/html/body/div[1]/div/div/div/div[2]/div/button[2]/span')\n",
    "\n",
    "            accept_cookies_button.click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_links(self) -> list:\n",
    "\n",
    "        recipes_container = self.driver.find_element(by=By.XPATH, value='/html/body/div[5]/div/form/div/div[4]/div[1]/div[1]/div')\n",
    "        recipes_list = recipes_container.find_elements(by=By.XPATH, value='./div')\n",
    "        link_list = []\n",
    "\n",
    "        for r_property in recipes_list:\n",
    "            a_tag = r_property.find_element(by=By.TAG_NAME, value='a')\n",
    "            link = a_tag.get_attribute('href')\n",
    "            link_list.append(link)\n",
    "        \n",
    "        return link_list\n",
    "\n",
    "    def strn(self) -> str:\n",
    "\n",
    "        strn = 'You can do it'\n",
    "\n",
    "        return strn\n",
    "\n",
    "\n",
    "    def get_recipe_info(self,j) -> dict:\n",
    "\n",
    "        recipe_name_l = '//*[@id=\"__next\"]/div[3]/main/div/section/div/div[3]/div[1]/h1'\n",
    "        Rating_l = '//*[@id=\"__next\"]/div[3]/main/div/section/div/div[3]/div[3]/div/a[1]/div/div/span[1]'\n",
    "        Number_of_Rating_l = '//*[@id=\"__next\"]/div[3]/main/div/section/div/div[3]/div[3]/div/a[1]/div/div/span[2]'\n",
    "        Prep_time_l = '//*[@id=\"__next\"]/div[3]/main/div/section/div/div[3]/ul[1]/li[1]/div/div[2]/ul/li[1]/span[2]/time'\n",
    "        Cook_time_l = '//*[@id=\"__next\"]/div[3]/main/div/section/div/div[3]/ul[1]/li[1]/div/div[2]/ul/li[2]/span[2]/time'\n",
    "        Making_l = '//*[@id=\"__next\"]/div[3]/main/div/section/div/div[3]/ul[1]/li[2]/div/div[2]'\n",
    "        Cook_Name_l = '//*[@id=\"__next\"]/div[3]/main/div/section/div/div[3]/div[2]/div/ul/li/a'\n",
    "        Recipe_des_l = '//*[@id=\"__next\"]/div[3]/main/div/section/div/div[3]/div[4]/p'\n",
    "        Image_Url_l = '//*[@id=\"__next\"]/div[3]/main/div/section/div/div[1]/div/div/picture/img'\n",
    "        kcal_l = '//*[@id=\"__next\"]/div[3]/main/div/section/div/div[3]/table/tbody[1]/tr[1]/td[3]'\n",
    "        fat_l = '//*[@id=\"__next\"]/div[3]/main/div/section/div/div[3]/table/tbody[1]/tr[2]/td[3]'\n",
    "        saturates_l = '//*[@id=\"__next\"]/div[3]/main/div/section/div/div[3]/table/tbody[1]/tr[3]/td[3]'\n",
    "        carbs_l = '//*[@id=\"__next\"]/div[3]/main/div/section/div/div[3]/table/tbody[1]/tr[4]/td[3]'\n",
    "        sugars_l = '//*[@id=\"__next\"]/div[3]/main/div/section/div/div[3]/table/tbody[2]/tr[1]/td[3]'\n",
    "        fiber_l = '//*[@id=\"__next\"]/div[3]/main/div/section/div/div[3]/table/tbody[2]/tr[2]/td[3]'\n",
    "        protein = '//*[@id=\"__next\"]/div[3]/main/div/section/div/div[3]/table/tbody[2]/tr[3]/td[3]'\n",
    "        salt = '//*[@id=\"__next\"]/div[3]/main/div/section/div/div[3]/table/tbody[2]/tr[4]/td[3]'\n",
    "\n",
    "        '''data = {'Recipe_Name': [], 'Rating': [], 'Number_of_Rating':[],'Prep_time': [], 'Cook_time': [],\n",
    "                    'Making': [], 'Cook_Name': [], 'Recipe_des': [], 'Image_ULR': [], 'kcal': [],\n",
    "                    'fat': [], 'saturates': [], 'carbs':[], 'sugars':[], 'fibre':[], 'protein':[],\n",
    "                    'salt': []}'''\n",
    "\n",
    "        if j == 1:\n",
    "            data = {'Recipe_Name': [self.__get_text(recipe_name_l)], 'Rating': [self.__get_text(Rating_l)], 'Number_of_Rating': [self.__get_text(Number_of_Rating_l)],'Prep_time': [self.__get_text(Prep_time_l)], 'Cook_time': [self.__get_text(Cook_time_l)],\n",
    "                    'Making': [self.__get_text(Making_l)], 'Cook_Name': [self.__get_text(Cook_Name_l)], 'Recipe_des': [self.__get_text(Recipe_des_l)], 'Image_Url': [self.__get_img_url(Image_Url_l)], 'kcal': [self.__get_text(kcal_l)],\n",
    "                    'fat': [self.__get_text(fat_l)], 'saturates': [self.__get_text(saturates_l)], 'carbs':[self.__get_text(carbs_l)], 'sugars':[self.__get_text(sugars_l)], 'fibre':[self.__get_text(fiber_l)], 'protein':[self.__get_text(protein)],\n",
    "                    'salt': [self.__get_text(salt)]}\n",
    "            \n",
    "        else:\n",
    "            data = {'Recipe_Name': self.__get_text(recipe_name_l), 'Rating': self.__get_text(Rating_l), 'Number_of_Rating': self.__get_text(Number_of_Rating_l),'Prep_time': self.__get_text(Prep_time_l), 'Cook_time': self.__get_text(Cook_time_l),\n",
    "                    'Making': self.__get_text(Making_l), 'Cook_Name': self.__get_text(Cook_Name_l), 'Recipe_des': self.__get_text(Recipe_des_l), 'Image_Url': self.__get_img_url(Image_Url_l), 'kcal': self.__get_text(kcal_l),\n",
    "                    'fat': self.__get_text(fat_l), 'saturates': self.__get_text(saturates_l), 'carbs':self.__get_text(carbs_l), 'sugars':self.__get_text(sugars_l), 'fibre':self.__get_text(fiber_l), 'protein':self.__get_text(protein),\n",
    "                    'salt': self.__get_text(salt)}\n",
    "\n",
    "        if data['kcal'] == 'N/A':\n",
    "            kcal_l = '//*[@id=\"__next\"]/div[3]/main/div/section/div/div[3]/table/tbody[1]/tr[1]/td[2]'\n",
    "            fat_l = '//*[@id=\"__next\"]/div[3]/main/div/section/div/div[3]/table/tbody[1]/tr[2]/td[2]'\n",
    "            saturates_l = '//*[@id=\"__next\"]/div[3]/main/div/section/div/div[3]/table/tbody[1]/tr[3]/td[2]'\n",
    "            carbs_l = '//*[@id=\"__next\"]/div[3]/main/div/section/div/div[3]/table/tbody[1]/tr[4]/td[2]'\n",
    "            sugars_l = '//*[@id=\"__next\"]/div[3]/main/div/section/div/div[3]/table/tbody[2]/tr[1]/td[2]'\n",
    "            fiber_l = '//*[@id=\"__next\"]/div[3]/main/div/section/div/div[3]/table/tbody[2]/tr[2]/td[2]'\n",
    "            protein = '//*[@id=\"__next\"]/div[3]/main/div/section/div/div[3]/table/tbody[2]/tr[3]/td[2]'\n",
    "            salt = '//*[@id=\"__next\"]/div[3]/main/div/section/div/div[3]/table/tbody[2]/tr[4]/td[2]'\n",
    "\n",
    "            data = {}\n",
    "            if j == 1:\n",
    "                data = {'Recipe_Name': [self.__get_text(recipe_name_l)], 'Rating': [self.__get_text(Rating_l)], 'Number_of_Rating': [self.__get_text(Number_of_Rating_l)],'Prep_time': [self.__get_text(Prep_time_l)], 'Cook_time': [self.__get_text(Cook_time_l)],\n",
    "                    'Making': [self.__get_text(Making_l)], 'Cook_Name': [self.__get_text(Cook_Name_l)], 'Recipe_des': [self.__get_text(Recipe_des_l)], 'Image_Url': [self.__get_img_url(Image_Url_l)], 'kcal': [self.__get_text(kcal_l)],\n",
    "                    'fat': [self.__get_text(fat_l)], 'saturates': [self.__get_text(saturates_l)], 'carbs':[self.__get_text(carbs_l)], 'sugars':[self.__get_text(sugars_l)], 'fibre':[self.__get_text(fiber_l)], 'protein':[self.__get_text(protein)],\n",
    "                    'salt': [self.__get_text(salt)]}\n",
    "            else:\n",
    "                data = {'Recipe_Name': self.__get_text(recipe_name_l), 'Rating': self.__get_text(Rating_l), 'Number_of_Rating': self.__get_text(Number_of_Rating_l),'Prep_time': self.__get_text(Prep_time_l), 'Cook_time': self.__get_text(Cook_time_l),\n",
    "                    'Making': self.__get_text(Making_l), 'Cook_Name': self.__get_text(Cook_Name_l), 'Recipe_des': self.__get_text(Recipe_des_l), 'Image_Url': self.__get_img_url(Image_Url_l), 'kcal': self.__get_text(kcal_l),\n",
    "                    'fat': self.__get_text(fat_l), 'saturates': self.__get_text(saturates_l), 'carbs':self.__get_text(carbs_l), 'sugars':self.__get_text(sugars_l), 'fibre':self.__get_text(fiber_l), 'protein':self.__get_text(protein),\n",
    "                    'salt': self.__get_text(salt)}\n",
    "        \n",
    "        return data\n",
    "\n",
    "\n",
    "    def __get_text(self, locator) -> str:\n",
    "        \n",
    "        try:\n",
    "            text = self.driver.find_element(by=By.XPATH, value = locator).text\n",
    "        except:\n",
    "            text = 'N/A'\n",
    "            \n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "    def __get_img_url(self, locator) -> str:\n",
    "        \n",
    "        try:\n",
    "            img_url = self.driver.find_element(by=By.XPATH, value=locator)\n",
    "            url = img_url.get_attribute('src')\n",
    "        except:\n",
    "            url = 'N/A'\n",
    "\n",
    "        return url\n",
    "\n",
    "    def merge_recipe_details(self,ard,rd) -> dict:\n",
    "\n",
    "        ard['recipe_url'].append(rd['recipe_url'])\n",
    "        ard['unique_ID'].append(rd['unique_ID'])\n",
    "\n",
    "        ard['Recipe_Name'].append(rd['Recipe_Name'])\n",
    "        ard['Rating'].append(rd['Rating'])\n",
    "        ard['Number_of_Rating'].append(rd['Number_of_Rating'])\n",
    "        ard['Prep_time'].append(rd['Prep_time'])\n",
    "        ard['Cook_time'].append(rd['Cook_time'])\n",
    "\n",
    "        ard['Making'].append(rd['Making'])\n",
    "        ard['Cook_Name'].append(rd['Cook_Name'])\n",
    "        ard['Recipe_des'].append(rd['Recipe_des'])\n",
    "        ard['Image_Url'].append(rd['Image_Url'])\n",
    "        ard['kcal'].append(rd['kcal'])\n",
    "\n",
    "        ard['fat'].append(rd['fat'])\n",
    "        ard['saturates'].append(rd['saturates'])\n",
    "        ard['carbs'].append(rd['carbs'])\n",
    "        ard['sugars'].append(rd['sugars'])\n",
    "        ard['fibre'].append(rd['fibre'])\n",
    "        ard['protein'].append(rd['protein'])\n",
    "        ard['salt'].append(rd['salt'])\n",
    "        ard['recipe_image'].append(rd['recipe_image'])\n",
    "\n",
    "        return ard\n",
    "\n",
    "    def get_image(self, image_url) -> str:\n",
    "        \n",
    "        try:\n",
    "            uid = image_url.split('?')[0]\n",
    "            uid = uid.split('/')[-1]\n",
    "            temp_image_location = '/tmp/' + uid\n",
    "            \n",
    "            urllib.request.urlretrieve(image_url, temp_image_location)\n",
    "\n",
    "            s3_url = self.aws.upload_file_method(temp_image_location, uid)\n",
    "            \n",
    "            return s3_url\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    def save_data(self, data) -> None:\n",
    "\n",
    "        try:\n",
    "            self.aws.write_to_rds(data)\n",
    "\n",
    "        except: \n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Scraper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/raghulsekar/Desktop/AI Core/Data Collection Pipeline/src/Main_Scraper.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/raghulsekar/Desktop/AI%20Core/Data%20Collection%20Pipeline/src/Main_Scraper.ipynb#ch0000000?line=0'>1</a>\u001b[0m \u001b[39m#Success on Collecting URL and Collecting Data - Running in headless mode\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/raghulsekar/Desktop/AI%20Core/Data%20Collection%20Pipeline/src/Main_Scraper.ipynb#ch0000000?line=2'>3</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/raghulsekar/Desktop/AI%20Core/Data%20Collection%20Pipeline/src/Main_Scraper.ipynb#ch0000000?line=3'>4</a>\u001b[0m     scraper \u001b[39m=\u001b[39m Scraper()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/raghulsekar/Desktop/AI%20Core/Data%20Collection%20Pipeline/src/Main_Scraper.ipynb#ch0000000?line=4'>5</a>\u001b[0m     scraper\u001b[39m.\u001b[39mload_page(\u001b[39m'\u001b[39m\u001b[39mhttps://www.bbcgoodfood.com/search/recipes/page/2/?q=Easy+dinner+recipes&sort=-relevance\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/raghulsekar/Desktop/AI%20Core/Data%20Collection%20Pipeline/src/Main_Scraper.ipynb#ch0000000?line=6'>7</a>\u001b[0m     number_of_page \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Scraper' is not defined"
     ]
    }
   ],
   "source": [
    "#Success on Collecting URL and Collecting Data - Running in headless mode\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = Scraper()\n",
    "    scraper.load_page('https://www.bbcgoodfood.com/search/recipes/page/2/?q=Easy+dinner+recipes&sort=-relevance')\n",
    "\n",
    "    number_of_page = 2\n",
    "    big_list = []\n",
    "    for i in range(number_of_page):\n",
    "        try:\n",
    "        \n",
    "            big_list.extend(scraper.get_links())\n",
    "            scraper.driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "            time.sleep(2)\n",
    "            next_button = scraper.driver.find_element(by=By.XPATH, value='/html/body/div[5]/div/form/div/div[4]/div[1]/div[2]/div/ul/li[7]/a/span')\n",
    "            next_button.click()\n",
    "        except:\n",
    "            pass\n",
    "    scraper.driver.close()\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    all_recipe_details = {}\n",
    "    \n",
    "    for link in big_list:\n",
    "        \n",
    "        i = i + 1\n",
    "        scraper.load_page(link)\n",
    "        recipe_details = {'recipe_url': [], 'unique_ID': []}\n",
    "\n",
    "        if i == 1:\n",
    "            recipe_details['recipe_url'] = [link]\n",
    "            recipe_details['unique_ID'] = [uuid.uuid4().hex]\n",
    "        else:\n",
    "            recipe_details['recipe_url'] = link\n",
    "            recipe_details['unique_ID'] = uuid.uuid4().hex\n",
    "\n",
    "        recipe_details = {**recipe_details, **scraper.get_recipe_info(i)}\n",
    "        if i == 1:\n",
    "            recipe_details['recipe_image'] = [scraper.get_image(recipe_details['Image_Url'][0])]\n",
    "            all_recipe_details = recipe_details\n",
    "        else:\n",
    "            recipe_details['recipe_image'] = scraper.get_image(recipe_details['Image_Url'])\n",
    "            all_recipe_details = scraper.merge_recipe_details(all_recipe_details,recipe_details)\n",
    "            \n",
    "        if i == 4:\n",
    "            break\n",
    "\n",
    "        pass \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Success on Collecting URL and Collecting Data -  ON process to add RDS\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = Scraper()\n",
    "    scraper.load_page('https://www.bbcgoodfood.com/search/recipes/page/2/?q=Easy+dinner+recipes&sort=-relevance')\n",
    "\n",
    "    number_of_page = 2\n",
    "    big_list = []\n",
    "    for i in range(number_of_page):\n",
    "        try:\n",
    "        \n",
    "            big_list.extend(scraper.get_links())\n",
    "            scraper.driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "            time.sleep(2)\n",
    "            next_button = scraper.driver.find_element(by=By.XPATH, value='/html/body/div[5]/div/form/div/div[4]/div[1]/div[2]/div/ul/li[7]/a/span')\n",
    "            next_button.click()\n",
    "        except:\n",
    "            pass\n",
    "    scraper.driver.close()\n",
    "\n",
    "    i = 1 #if saving sata to rds then i let it be 1\n",
    "\n",
    "    all_recipe_details = {}\n",
    "    \n",
    "    for link in big_list:\n",
    "        \n",
    "        i = i + 1\n",
    "        scraper.load_page(link)\n",
    "        recipe_details = {'recipe_url': [], 'unique_ID': []}\n",
    "\n",
    "        if i == 1:\n",
    "            recipe_details['recipe_url'] = [link]\n",
    "            recipe_details['unique_ID'] = [uuid.uuid4().hex]\n",
    "        else:\n",
    "            recipe_details['recipe_url'] = link\n",
    "            recipe_details['unique_ID'] = uuid.uuid4().hex\n",
    "\n",
    "        recipe_details = {**recipe_details, **scraper.get_recipe_info(i)}\n",
    "        if i == 1:\n",
    "            recipe_details['recipe_image'] = [scraper.get_image(recipe_details['Image_Url'][0])]\n",
    "            scraper.save_data(recipe_details)\n",
    "            all_recipe_details = recipe_details\n",
    "        else:\n",
    "            recipe_details['recipe_image'] = scraper.get_image(recipe_details['Image_Url'])\n",
    "            scraper.save_data(recipe_details)\n",
    "            # all_recipe_details = scraper.merge_recipe_details(all_recipe_details,recipe_details)\n",
    "            \n",
    "        if i == 5:\n",
    "            break\n",
    "\n",
    "        pass \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "recipe_url          object\n",
       "unique_ID           object\n",
       "Recipe_Name         object\n",
       "Rating              object\n",
       "Number_of_Rating    object\n",
       "Prep_time           object\n",
       "Cook_time           object\n",
       "Making              object\n",
       "Cook_Name           object\n",
       "Recipe_des          object\n",
       "Image_Url           object\n",
       "kcal                object\n",
       "fat                 object\n",
       "saturates           object\n",
       "carbs               object\n",
       "sugars              object\n",
       "fibre               object\n",
       "protein             object\n",
       "salt                object\n",
       "recipe_image        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_recipe_details_df = pd.DataFrame.from_dict(all_recipe_details)\n",
    "a = all_recipe_details_df.dtypes\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0           int64\n",
       "recipe_url          object\n",
       "unique_ID           object\n",
       "Recipe_Name         object\n",
       "Rating              object\n",
       "Number_of_Rating    object\n",
       "Prep_time           object\n",
       "Cook_time           object\n",
       "Making              object\n",
       "Cook_Name           object\n",
       "Recipe_des          object\n",
       "Image_Url           object\n",
       "kcal                 int64\n",
       "fat                 object\n",
       "saturates           object\n",
       "carbs               object\n",
       "sugars              object\n",
       "fibre               object\n",
       "protein             object\n",
       "salt                object\n",
       "recipe_image        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/raghulsekar/Desktop/recipe_details.csv')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif __name__ == \"__main__\":\\n    scraper = Scraper()\\n    i = 0\\n    \\n    all_recipe_details = {}\\n\\n    for link in big_list:\\n        i = i + 1\\n        scraper.load_page(link)\\n\\n        recipe_details = {\\'recipe_url\\': [], \\'unique_ID\\': []}\\n        if i == 1:\\n            recipe_details[\\'recipe_url\\'] = [link]\\n            recipe_details[\\'unique_ID\\'] = [uuid.uuid4().hex]\\n        else:\\n            recipe_details[\\'recipe_url\\'] = link\\n            recipe_details[\\'unique_ID\\'] = uuid.uuid4().hex\\n            \\n        recipe_details = {**recipe_details, **scraper.get_recipe_info(i)}\\n\\n        if i == 1:\\n            recipe_details[\\'recipe_image\\'] = [scraper.get_image(recipe_details[\\'Image_Url\\'][0])]\\n            all_recipe_details = recipe_details\\n        else:\\n            recipe_details[\\'recipe_image\\'] = scraper.get_image(recipe_details[\\'Image_Url\\'])\\n            all_recipe_details = scraper.merge_recipe_details(all_recipe_details,recipe_details)\\n            \\n        if i == 5:\\n            break\\n\\n    print(all_recipe_details)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sucess on collecting Data \n",
    "\n",
    "'''\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = Scraper()\n",
    "    i = 0\n",
    "    \n",
    "    all_recipe_details = {}\n",
    "\n",
    "    for link in big_list:\n",
    "        i = i + 1\n",
    "        scraper.load_page(link)\n",
    "\n",
    "        recipe_details = {'recipe_url': [], 'unique_ID': []}\n",
    "        if i == 1:\n",
    "            recipe_details['recipe_url'] = [link]\n",
    "            recipe_details['unique_ID'] = [uuid.uuid4().hex]\n",
    "        else:\n",
    "            recipe_details['recipe_url'] = link\n",
    "            recipe_details['unique_ID'] = uuid.uuid4().hex\n",
    "            \n",
    "        recipe_details = {**recipe_details, **scraper.get_recipe_info(i)}\n",
    "\n",
    "        if i == 1:\n",
    "            recipe_details['recipe_image'] = [scraper.get_image(recipe_details['Image_Url'][0])]\n",
    "            all_recipe_details = recipe_details\n",
    "        else:\n",
    "            recipe_details['recipe_image'] = scraper.get_image(recipe_details['Image_Url'])\n",
    "            all_recipe_details = scraper.merge_recipe_details(all_recipe_details,recipe_details)\n",
    "            \n",
    "        if i == 5:\n",
    "            break\n",
    "\n",
    "    print(all_recipe_details)\n",
    "'''\n",
    "\n",
    "         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('DCP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5aeba3a6c10395433c3ffaf8b273e518cbb9ec969e669a41f720a9baabc2caf5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
